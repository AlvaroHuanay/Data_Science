{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0dae232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95af4a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'example.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-66f8faa6e079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;31m#########################################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"example.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;31m#pd.read_sql(\"example.sql\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;31m#pd.read_html(\"example.html\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'example.csv'"
     ]
    }
   ],
   "source": [
    "#Pandas fundamentals\n",
    "\n",
    "a=np.random.seed(101) #introduce random seed \n",
    "df=pd.DataFrame(randn(5,4),[\"A\",\"B\",\"C\",\"D\",\"E\"], [\"W\",\"X\",\"Y\",\"Z\"]) #create dataframe with random values (5,4)\n",
    "df[\"new\"]=(df[\"W\"]+df[\"X\"])*randn() #create row \"new\"\n",
    "df=df.drop(\"new\", axis=1) #delete column \"new\"\n",
    "df.loc[\"C\"] #show row \"C\"\n",
    "df.iloc[2]  #show row \"C\"\n",
    "df.loc[\"B\",\"W\"] #show row \"C\" + column \"W\"\n",
    "df.loc[[\"E\",\"D\"],[\"W\",\"Y\"]] #show rows E and D with columns W and Y\n",
    "df[df[\"W\"]>0] #show the rows were \"W\">0\n",
    "df[df[\"W\"]>0][\"X\"] #from rows with W>0, show column \"X\" \n",
    "True and True #\"and\" operator only operates with 1 item, NOT multiple items\n",
    "df[(df[\"W\"]>0) & (df[\"Y\"]<0)] #return rows that obbey this condition (rows which have this element-column \"True\")\n",
    "df.reset_index() #Resets the index. Adds an \"index\" title to the left side of the table and a counter from 0\n",
    "\n",
    "newindex=\"CA NY WY OR CO\".split() #create a list\n",
    "df[\"States\"]=newindex #add a new column named as \"States\"\n",
    "df.set_index(\"States\") #Set \"States\" as index\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#How to create a multi-level table with pandas\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "outside=[\"G1\",\"G1\",\"G1\", \"G2\", \"G2\", \"G2\"]\n",
    "inside=[1,2,3,1,2,3]\n",
    "hier_index= list(zip(outside,inside)) #list a zip. Zip (make tuples of (a0,b0),(a1,b1)...)\n",
    "hier_index= pd.MultiIndex.from_tuples(hier_index) #Converts list of tuples to MultiIndex\n",
    "df=pd.DataFrame(randn(6,2),hier_index,[\"A\", \"B\"]) #Multi-level table\n",
    "#this table has no index in G_i, 1,2,3... If we would like to add an index\n",
    "df.index.names=[\"Groups\",\"Num\"] #Add names to the first 2 columns\n",
    "df.loc[\"G1\"]\n",
    "df.loc[\"G1\"].loc[2][\"A\"] #To grab an element: Row \"G1\", index (Num) \"2\", column A\n",
    "df.xs(1,level=\"Num\") #To access directly to the \"Num\" column where its value is =\"1\" \"xs\"=cross site\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#Operate with missing data in a pandas\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "d={\"A\":[1,2,np.nan],\"B\":[5,np.nan,np.nan], \"C\":[1,2,3]} #create a dictionary\n",
    "df=pd.DataFrame(d) #create a dataframe with this dictionary (there are many ways to create a df! See the ones before)\n",
    "df.dropna() #deletes all rows that have at least 1 \"NaN\"\n",
    "df.dropna(axis=1) #same as before but with columns\n",
    "df.dropna(thresh=2) #delete the rows that have at least 2 \"NaN\"\n",
    "df.fillna(value=\"FILL VALUE\") #fills \"NaN\" value with that string\n",
    "df[\"A\"].fillna(value=df[\"A\"].mean()) #fill the \"NaN\" value of \"A\" with the mean of the column (this has a statistical bacjground behind)\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#Grouping\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],\n",
    "       'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],\n",
    "       'Sales':[200,120,340,124,243,350]}\n",
    "df=pd.DataFrame(data) #create dataframe\n",
    "df.groupby(\"Company\") #group all the values of the table that are not strings. If there is a string/character in the column, it will be deleted\n",
    "df.groupby(\"Company\").max() #group all max values of the \"Company\". Idem with .min()\n",
    "df.groupby(\"Company\").describe().transpose()[\"FB\"] # main statistics for a specific \"Company\"\n",
    "df.groupby(\"Company\").describe().transpose() #main statistics for all companies\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#Merging, Joining and Concatenating dataframes\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "#Concatenating\n",
    "df1=pd.DataFrame({\"A\":[\"A0\",\"A1\",\"A2\",\"A3\"],\n",
    "                \"B\":[\"B0\",\"B1\",\"B2\",\"B3\"],\n",
    "                \"C\":[\"C0\",\"C1\",\"C2\",\"C3\"],\n",
    "                \"D\":[\"D0\",\"D1\",\"D2\",\"D3\"]},\n",
    "                index=[0,1,2,3]) \n",
    "df2=pd.DataFrame({\"A\":[\"A4\",\"A5\",\"A6\",\"A7\"],\n",
    "                \"B\":[\"B4\",\"B5\",\"B6\",\"B7\"],\n",
    "                \"C\":[\"C4\",\"C5\",\"C6\",\"C7\"],\n",
    "                \"D\":[\"D4\",\"D5\",\"D6\",\"D7\"]},\n",
    "                index=[4,5,6,7])\n",
    "df3=pd.DataFrame({\"A\":[\"A8\",\"A9\",\"A10\",\"A11\"],\n",
    "                \"B\":[\"B8\",\"B9\",\"B10\",\"B11\"],\n",
    "                \"C\":[\"C8\",\"C9\",\"C10\",\"C11\"],\n",
    "                \"D\":[\"D8\",\"D9\",\"D10\",\"D11\"]},\n",
    "                index=[8,9,10,11])\n",
    "df_concatenate=pd.concat([df1,df2,df3]) #joins the rows together\n",
    "df_concatenate=pd.concat([df1,df2,df3], axis=1) #joins the columns together\n",
    "#Merging\n",
    "\n",
    "#case1\n",
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "   \n",
    "right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                          'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                          'D': ['D0', 'D1', 'D2', 'D3']}) \n",
    "df_merged=pd.merge(left,right,how=\"inner\", on=\"key\")\n",
    "\n",
    "left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "                     'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "                        'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                        'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "#case2  \n",
    "right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "                               'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "                                  'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                                  'D': ['D0', 'D1', 'D2', 'D3']})\n",
    "df_merged2=pd.merge(left, right, on=['key1', 'key2'])\n",
    "#case3\n",
    "df_merged3=pd.merge(left, right, how='outer', on=['key1', 'key2'])\n",
    "#case4\n",
    "df_merged4=pd.merge(left, right, how='left', on=['key1', 'key2'])\n",
    "\n",
    "\n",
    "\n",
    "#Joining\n",
    "\n",
    "left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                     'B': ['B0', 'B1', 'B2']},\n",
    "                      index=['K0', 'K1', 'K2']) \n",
    "\n",
    "right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D2', 'D3']},\n",
    "                      index=['K0', 'K2', 'K3'])\n",
    "#case1\n",
    "df_left_joined_inner=left.join(right) #only shows k0,k1,k2. NOT k3. Because it joins left WITH right\n",
    "df_right_joined_inner=right.join(left) #only shows k0,k2,k3. NOT k1. Because it joins right WITH left\n",
    "#case2\n",
    "df_left_joined_outer=left.join(right,how=\"outer\") #joins left WITH right, shows all elements and NaNs\n",
    "df_right_joined_outer=right.join(left,how=\"outer\") #joins right WITH left, shows all elements and NaNs\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#Operations (using pandas)\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444],'col3':['abc','def','ghi','xyz']})\n",
    "df_unique_values=df[\"col2\"].unique() #shows an numpy n-D array of unique values of col2\n",
    "df_number_unique_values=df[\"col2\"].nunique() #shows the NUMBER of unique values in this column\n",
    "df_unique_num=df[\"col2\"].value_counts() #how many times each unique number appeared in that column\n",
    "df_greater_than_2=df[df[\"col1\"]>2] #returns all the rows of the dataframe where the values of col1>2\n",
    "\n",
    "#create a function and use it as an operation\n",
    "def times2(x):\n",
    "    return x*2\n",
    "df[\"col1\"].apply(times2) #previously defining a function\n",
    "df[\"col2\"].apply(lambda x: x*2) #using a lambda expresion (THE BEST!)\n",
    "#keywords\n",
    "df.columns #without \"()\"\n",
    "df.index #without \"()\"\n",
    "df.sort_values(by=\"col2\") #sort in ascending order\n",
    "df.isnull() #indicates which values are null\n",
    "\n",
    "#pivot tables\n",
    "\n",
    "data = {'A':['foo','foo','foo','bar','bar','bar'],\n",
    "     'B':['one','one','two','two','one','one'],\n",
    "       'C':['x','y','x','y','x','y'],\n",
    "       'D':[1,3,2,5,4,1]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#Creates a pivot table that counts the number of \"one\" and \"two\" for each \"A\" element\n",
    "df_pivot=df.pivot_table(values=\"D\", index=[\"A\",\"B\"], columns=[\"C\"]) # values to count from \"D\". \"A\" and \"B\" will be de new indexes. Columns are determined by \"C\" elements. Count the number of values.\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#Data input and output (.csv, .sql, .html, excel)\n",
    "#########################################################################################################\n",
    "\n",
    "df=pd.read_csv(\"example.csv\")\n",
    "#pd.read_sql(\"example.sql\")\n",
    "#pd.read_html(\"example.html\")\n",
    "#pd.read_excel(\"example.xlsx\")\n",
    "\n",
    "df.to_csv(\"My_output.csv\", index=False) #To NOT to save the index column which is useless sometimes. Python assigns an index by itself. We do not want redundancy\n",
    "\n",
    "\n",
    "#To treat an excel file\n",
    "#Pandas thinks a workbook is a bunch of sheets and each sheet is a dataframe\n",
    "Excel_Sample=pd.read_excel(\"Excel_Sample.xlsx\", sheetname=\"Sheet1\") #to read a dataframe from an EXCEL FILE\n",
    "Excel_Sample=df.to_excel(\"Excel_Sample2.xlsx\", sheet_name=\"NewSheet\") #to save a dataframe to an EXCEL FILE\n",
    "\n",
    "#To treat an html file\n",
    "#Creates and enormous list, and each element in the list is a dataframe\n",
    "data=pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')\n",
    "data[0] #first dataframe of the list\n",
    "\n",
    "#To treat an sql file\n",
    "from sqlalchemy import create_engine\n",
    "engine=create_engine(\"sqlite:///:memory:\") #to create a sql engine in memory (to create a database). It will use the last dataframe used (previous exercise) but from an sql engine!\n",
    "df.to_sql(\"my_table\", engine)\n",
    "sqldf=pd.read_sql(\"my_table\", con=engine)\n",
    "sqldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d39f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
